* DataCamp 

** Intermediate python for data science

*** Matplotlib  
**** Basic plotting
Import the matplotlib library normally as an alias of plt, and to plot a simple line chart we do: 

#+BEGIN_SRC python 
import matplotlib.pyplot as plt 
plt.plot(x, y)
plt.show() 
#+END_SRC 

plt.plot will create a line chart, if we only want to show the point simply use `plt.scatter(x, y)` instead. 

To put some of the axis in log-scale simple 
#+BEGIN_SRC python
plt.xscale('log') 
#+END_SRC

on scatter plot we can pass the s argument to make some of the data points bigger and showing 3 dimensions 
for each point (x, y and scale) 

#+BEGIN_SRC python 
plt.scatter(gdp_cap, life_exp, s = population) 
#+END_SRC 
**** Histogram 
#+BEGIN_SRC 
plt.hist(x, bins=10) 
plt.show() 
#+END_SRC 
**** Customization 

- Name the axis 
- Title of the chart 

#+BEGIN_SRC python
plt.title('title of the chart') 
plt.xlabel('label x') 
plt.ylabel('label y') 
#+END_SRC 

- add ticks to the axis (meaning we can configure and give name, to the axis separator ticks) 

#+BEGIN_SRC python 
plt.yticks([0, 1, 2], ['one', 'two', 'three']) 
#+END_SRC 

On this example, we have configured to have ticks on the y-axis on values 0 1 and 2. and on the chart 
instead of showing the value of the axis, it will show the names provided 

- Change colors: on most plt objects we can pass a c parameter (for color ;)), is a list that should match 
the size of x and y.

#+BEGIN_SRC python 
plt.scatter(x, y, c=['red', 'blue']) 
#+END_SRC 

- Add text to the chart, we can add text elements to the chart. 

to do so we use the text method with the x,y position in terms of the data we already plotted 

#+BEGIN_SRC python 
plt.scatter(gdp_cap, life_exp)
plt.text(1550, 71, 'India') 
#+END_SRC 


- Add the grid: after we done all the plotting thing, before use the plt.show method we can call the plt.grid() 
mehtod to make the axis ticks to become an actual grid.

*** Pandas
    

To create a pandas dataframe, we can use dictionaries to load the info inside the df
the dictionaries must have the following structure:

#+BEGIN_SRC python
  import pandas as pd

  data = {
      "col1": [1, 2, 3],
      "col2": [4, 5, 6]
  }

  df = pd.DataFrame(data)


#+END_SRC

#+RESULTS:

Load data from the csv file
#+BEGIN_SRC python

pd.read_csv('file.csv', index_col = 0)
#+END_SRC

To select a column we can use the [] operator, with the column name and will return a
Serie object

If we want a dataframe back we can use [] with a list of columns we want

#+BEGIN_SRC python
df["country"]  ## returns a serie
df[["country"]] ## returns a dataframe with a single column
df[["country", "population"]] ## returns a dataframe with two columns 

#+END_SRC


To select rows we can use [] also but with ints:

#+BEGIN_SRC python
df[0] ## first rows
df[0:9] ## select 10 first rows
#+END_SRC

To select rows from the dataframe we can use loc and iloc functions:

- loc means to select elements from the dataframe using labels
- iloc: also is for selecting from the dataframe but by position


#+BEGIN_SRC python
df.loc[["ROW1", "ROW2"], ["col1", "col2"]] ## selects row1 and row2 from the dataset with col1 and col2
df.loc[[:, ["col1", "col2"]] ## all rows with only the col1 and col2
#+END_SRC


*** Pandas filtering
    

    1. apply a boolean to the dataframe to obtain a Serie of boolean
    2. use this serie to select rows of the dataframe

#+BEGIN_SRC python
  m = df["area"] > 15
  data_filtered = df[m]
#+END_SRC


** supervised learning with scikit

#+BEGIN_SRC python
import matplotlib.pyplot as plt
plt.style.use('ggplot')
#+END_SRC
   
*** EDA (Exploratory data analysis)
 #+BEGIN_SRC python
 df.describe() ## shows for each variable/column in the df count, mean and percent
 df.info() ## shows each colum how many values have n/A and the types
 #+END_SRC

 visual EDA: How to make grid of charts from a pandas dataframe,
 
#+BEGIN_SRC 
pd.scatter_matrix(df, c = y, figsize = [8, 8], s =150, marker = 'D')
#+END_SRC

also we can use seaborns heatmap function and the df.corr function like this

#+BEGIN_SRC python
sns.heatmap(df.corr(), square=True, cmap='RdYlGn')
#+END_SRC

*** Knn

#+BEGIN_SRC python
# Import KNeighborsClassifier from sklearn.neighbors
from sklearn.neighbors import KNeighborsClassifier

# Create arrays for the features and the response variable
y = df['party'].values
X = df.drop('party', axis=1).values

# Create a k-NN classifier with 6 neighbors
knn = KNeighborsClassifier(n_neighbors = 6)

# Fit the classifier to the data
knn.fit(X, y)
#+END_SRC

df["col1"].values -> returns a numpy array with the values of each row for this column

preditcting using knn
#+BEGIN_SRC python
knn = KNeighborsClassifier(n_neighbors = 6)

# Fit the classifier to the data
knn.fit(X, y)

# Predict the labels for the training data X
y_pred = knn.predict(X)
#+END_SRC

*** Measuring accuracy (train/test split)
    
    we can use the train_test_split function to split arrays in train and test set

    also we can use the **statify** argument, to ensure that the distribution on the 
    train and test set are the same that in the original dataset
    
#+BEGIN_SRC python
  from sklearn.model_selection import train_test_split

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                      stratify = y)
#+END_SRC
    

To get the accuracy we can use the score method of the algorithm like this

#+BEGIN_SRC python
knn.score(X_test, y_test)
#+END_SRC

*** Cross Validation

#+BEGIN_SRC python
  from sklearn.model_selection import cross_val_score

  reg = linear_model.LinearRegression() ## any classifier/regressor will work

  cv_results = cross_val_score(reg, X, y, cv = 10)

#+END_SRC


*** How good is the model?

for the spam problem (decide if a email is spam or not) the distribution of the
data might be like 99% are real and 1% are spam.

We can build a algorithm that has a 99% accuracy by just predict always real...
so acurracy might not be always the best metric.

for binary classifiers we can use the confusion matrix

from the matrix we can get the:

- acurracy: 
- precision: tp / (tp + fp)  ## of all emails predicted as spam, how many they were spam
- recall: tp / tp + fn   ## sensitivy
- F1 score = 2 * (precision * recall) / (precision + recall)

#+BEGIN_SRC python
# Import necessary modules
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Create training and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Instantiate a k-NN classifier: knn
knn = KNeighborsClassifier(n_neighbors = 6)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Predict the labels of the test data: y_pred
y_pred = knn.predict(X_test)

# Generate the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))


#+END_SRC

*** ROC and Area on the curve ROC Metric

I did not take any notes, but is insteresting to review, please take notes later :)

*** GridSearch

To search for the hyper parameters, we create dictionary and in it, for each
 parameters of the function that we want to search, we create a new entry with the 
same name as the keyword argument, and as a value the range of values we want to try

#+BEGIN_SRC python
  from sklearn.model_selection import GridSearchCV
  param_grid = {
    'n_neighbors': np.arange(1, 50)
  }

  knn = KNeighborsClassifier()

  knn_cv = GridSearchCV(knn, param_grid, cv=5)

  knn_cv.fit(X, y)

  knn_cv.best_params_
  knn_cv.best_score_

#+END_SRC


Also we can use the RandomizedSearchCV, which doesn't test all the possibilities, this
is because doing the full GridSearch is expensive.


a complete example using grid search, train etc..

#+BEGIN_SRC python
# Import necessary modules
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV, train_test_split

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Create the hyperparameter grid
l1_space = np.linspace(0, 1, 30)
param_grid = {'l1_ratio': l1_space}

# Instantiate the ElasticNet regressor: elastic_net
elastic_net = ElasticNet()

# Setup the GridSearchCV object: gm_cv
gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)

# Fit it to the training data
gm_cv.fit(X_train, y_train)

# Predict on the test set and compute metrics
y_pred = gm_cv.predict(X_test)
r2 = gm_cv.score(X_test, y_test)
mse = mean_squared_error(y_test, y_pred)
print("Tuned ElasticNet l1 ratio: {}".format(gm_cv.best_params_))
print("Tuned ElasticNet R squared: {}".format(r2))
print("Tuned ElasticNet MSE: {}".format(mse))
#+END_SRC

*** Preprocessing data

When we have categorical variables, we have to convert them to multiple
boolean variables.

For example, if we got a categorical variable that can have the values (US, EU, ASIA) 
then we have to create 3 boolean features.

IS_US, IS_EU, IS_ASIA

to convert this categorical variables, we can use:

1. scikit-learn: OneHotEncoder()
2. pandasL get_dummies()

#+BEGIN_SRC python
df_no_categorical = pd.get_dummies(df)
#+END_SRC

this creates a new dataframe with the colums origin_asia, origin_eu, origin_us

since one of the columns is redundant (if not is US and EU, means that is Asia)
we can drop the column Asia

#+BEGIN_SRC python
df_no_categorical = df_no_categorical.drop('origin_Asia', axis = 1)
#+END_SRC

*** Missing data

    - if we got some data that seems missing we can replace the values of missing (in this case 0)
by nans

#+BEGIN_SRC python
df.bmi.replace(0, np.nan, inplace=True)
#+END_SRC

or...

#+BEGIN_SRC python
df[df == '?'] = np.nan
#+END_SRC

    - Now, if we don't want the rows that have some column value = nan we can drop them like that

#+BEGIN_SRC python
df = df.dropna()
#+END_SRC

    - other option, is to fill the na values of the column with the mean of the column itself
 
#+BEGIN_SRC python
from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', stategy='mean', axis=0)
imp.fit(X)
X = imp.transform(X)
#+END_SRC

another way is using pipeline, for use in a pipeline each step but the last must be an transformer
and the last one have to be a estimator (transformer means that can call the method transform)


#+BEGIN_SRC python
  from sklearn.pipeline import Pipeline
  from sklearn.preprocessing import Imputer

  imp = Imputer(missing_values='NaN', stategy='mean', axis=0)
  logreg = LogisticRegression()

  steps = [('imputation', imp)
           ('logistic_regression', logreg)]

  pipeline = Pipeline(steps)

  X_train, X_test, y_train, y_test = train_test_split(X, y)

  pipeline.fit(X_train, y_train)
  y_pred=  pipeline.predict(X_test)
  pipeline.score(X_test, y_test)

#+END_SRC

    - Centering and scaling, (normalizing): subtract the mean and divide variance
#+BEGIN_SRC python
from sklearn.preprocessing import scale
X_scaled = scale(X)
#+END_SRC

or... in a pipeline

#+BEGIN_SRC python
  import sklean.preprocessing import StandardScaler

  steps = [('scaler', StandardScaler()),
           ('knn', KNeighborsClassifier())]
#+END_SRC


    - Pipeline with CV grid search, the paramets have to be specified by: ${step_name}__${parameter_name}

#+BEGIN_SRC python
# Setup the pipeline
steps = [('scale', StandardScaler()),
         ('SVM', SVC())]

pipeline = Pipeline(steps)

# Specify the hyperparameter space
parameters = {'SVM__C':[1, 10, 100],
              'SVM__gamma':[0.1, 0.01]}

# Create train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

# Instantiate the GridSearchCV object: cv
cv = GridSearchCV(pipeline, parameters)

# Fit to the training set
cv.fit(X_train, y_train)

# Predict the labels of the test set: y_pred
y_pred = cv.predict(X_test)

# Compute and print metrics
print("Accuracy: {}".format(cv.score(X_test, y_test)))
print(classification_report(y_test, y_pred))
print("Tuned Model Parameters: {}".format(cv.best_params_))


#+END_SRC

** Natural language processing Fundamentals in Python

*** Introduction to RegEx
    - 
